{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba45624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Libraries Imported Successfully ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Classification Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4091511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Loading Data and Engineering Features ---\n",
      "‚úÖ New features created.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 2: Loading Data and Engineering Features ---\")\n",
    "df = pd.read_csv('1hour.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y:%m:%d %H:%M:%S')\n",
    "df = df.sort_values('timestamp').set_index('timestamp')\n",
    "\n",
    "# --- Create Target Variables for BOTH models ---\n",
    "# Classification Target: 1 if the price goes up, 0 if it goes down.\n",
    "df['Target_Movement'] = (df['close'].shift(-1) > df['close']).astype(int)\n",
    "# Regression Target: The actual closing price of the next hour.\n",
    "df['Target_Close'] = df['close'].shift(-1)\n",
    "\n",
    "# --- Engineer New Features ---\n",
    "df['price_change'] = df['close'] - df['open']\n",
    "df['high_low_diff'] = df['high'] - df['low']\n",
    "for window in [6, 12, 24]:\n",
    "    df[f'rolling_mean_close_{window}h'] = df['close'].rolling(window=window).mean()\n",
    "    df[f'rolling_std_close_{window}h'] = df['close'].rolling(window=window).std()\n",
    "df['rsi'] = ta.momentum.RSIIndicator(close=df['close'], window=14).rsi()\n",
    "df['macd'] = ta.trend.MACD(close=df['close']).macd()\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)\n",
    "df['dayofweek_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7)\n",
    "print(\"‚úÖ New features created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1e05df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Lagging All Features to Prevent Data Leakage ---\n",
      "‚úÖ All features lagged. Data is ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 3: Lagging All Features to Prevent Data Leakage ---\")\n",
    "feature_cols_to_lag = [col for col in df.columns if 'Target' not in col]\n",
    "for col in feature_cols_to_lag:\n",
    "    df[f'{col}_lag1'] = df[col].shift(1)\n",
    "df.dropna(inplace=True)\n",
    "print(\"‚úÖ All features lagged. Data is ready for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992c7db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Defining Final Feature Set and Targets ---\n",
      "We have 17 features for our models.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 4: Defining Final Feature Set and Targets ---\")\n",
    "FINAL_FEATURE_COLS = [col for col in df.columns if '_lag1' in col]\n",
    "CLS_TARGET_COL = 'Target_Movement'\n",
    "REG_TARGET_COL = 'Target_Close'\n",
    "\n",
    "X = df[FINAL_FEATURE_COLS]\n",
    "y_cls = df[CLS_TARGET_COL]\n",
    "y_reg = df[REG_TARGET_COL]\n",
    "print(f\"We have {len(FINAL_FEATURE_COLS)} features for our models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0efdc443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "üöÄ STARTING CLASSIFICATION MODEL PIPELINE üöÄ\n",
      "==================================================\n",
      "\n",
      "--- Step 5a: Evaluating Classification Models with Normal Parameters ---\n",
      "  Accuracy of Random Forest: 49.42%\n",
      "  Accuracy of Decision Tree: 50.31%\n",
      "  Accuracy of XGBoost: 50.44%\n",
      "\n",
      "üèÜ Best Baseline Classifier: 'XGBoost'\n",
      "\n",
      "--- Step 5b: Performing Hyperparameter Tuning for XGBoost ---\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "--- Step 5c: Final Evaluation of Tuned Classification Model ---\n",
      "Initial Accuracy of XGBoost: 50.44%\n",
      "**Final Tuned Accuracy of XGBoost: 52.27%**\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Down üîΩ       0.52      0.45      0.48      5285\n",
      "        Up üîº       0.53      0.60      0.56      5487\n",
      "\n",
      "    accuracy                           0.52     10772\n",
      "   macro avg       0.52      0.52      0.52     10772\n",
      "weighted avg       0.52      0.52      0.52     10772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"üöÄ STARTING CLASSIFICATION MODEL PIPELINE üöÄ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 5a. Split data for Classification ---\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X, y_cls, test_size=0.2, random_state=42, stratify=y_cls)\n",
    "\n",
    "# --- 5b. Find Best Baseline Classification Model ---\n",
    "print(\"\\n--- Step 5a: Evaluating Classification Models with Normal Parameters ---\")\n",
    "cls_models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "baseline_accuracies = {}\n",
    "for name, model in cls_models.items():\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('classifier', model)])\n",
    "    pipeline.fit(X_train_cls, y_train_cls)\n",
    "    y_pred = pipeline.predict(X_test_cls)\n",
    "    accuracy = accuracy_score(y_test_cls, y_pred)\n",
    "    baseline_accuracies[name] = accuracy\n",
    "    print(f\"  Accuracy of {name}: {accuracy:.2%}\")\n",
    "\n",
    "best_cls_model_name = max(baseline_accuracies, key=baseline_accuracies.get)\n",
    "print(f\"\\nüèÜ Best Baseline Classifier: '{best_cls_model_name}'\")\n",
    "\n",
    "# --- 5c. Hyperparameter Tune the Best Classification Model ---\n",
    "print(f\"\\n--- Step 5b: Performing Hyperparameter Tuning for {best_cls_model_name} ---\")\n",
    "cls_param_grids = {\n",
    "    \"Random Forest\": {'classifier__n_estimators': [100, 200], 'classifier__max_depth': [10, 20]},\n",
    "    \"Decision Tree\": {'classifier__max_depth': [10, 20, 30]},\n",
    "    \"XGBoost\": {'classifier__n_estimators': [100, 200], 'classifier__learning_rate': [0.05, 0.1], 'classifier__max_depth': [3, 5]}\n",
    "}\n",
    "chosen_cls_model = cls_models[best_cls_model_name]\n",
    "chosen_cls_grid = cls_param_grids[best_cls_model_name]\n",
    "final_cls_pipeline = Pipeline([('scaler', StandardScaler()), ('classifier', chosen_cls_model)])\n",
    "grid_search_cls = GridSearchCV(final_cls_pipeline, chosen_cls_grid, cv=3, n_jobs=-1, scoring='accuracy', verbose=1)\n",
    "grid_search_cls.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "# --- 5d. Final Evaluation of Tuned Classification Model ---\n",
    "print(\"\\n--- Step 5c: Final Evaluation of Tuned Classification Model ---\")\n",
    "final_cls_model = grid_search_cls.best_estimator_\n",
    "y_pred_cls_final = final_cls_model.predict(X_test_cls)\n",
    "final_accuracy = accuracy_score(y_test_cls, y_pred_cls_final)\n",
    "print(f\"Initial Accuracy of {best_cls_model_name}: {baseline_accuracies[best_cls_model_name]:.2%}\")\n",
    "print(f\"**Final Tuned Accuracy of {best_cls_model_name}: {final_accuracy:.2%}**\")\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "print(classification_report(y_test_cls, y_pred_cls_final, target_names=['Down üîΩ', 'Up üîº']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b50be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "üöÄ STARTING REGRESSION MODEL PIPELINE üöÄ\n",
      "==================================================\n",
      "\n",
      "--- Step 6a: Evaluating Regression Models with Normal Parameters ---\n",
      "  R¬≤ Score of Linear Regression: 0.9988\n",
      "  R¬≤ Score of Random Forest: 0.9961\n",
      "  R¬≤ Score of XGBoost: 0.9867\n",
      "\n",
      "üèÜ Best Baseline Regressor: 'Linear Regression'\n",
      "\n",
      "--- Step 6b: Performing Hyperparameter Tuning for Linear Regression ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "--- Step 6c: Final Evaluation of Tuned Regression Model ---\n",
      "Initial R¬≤ Score of Linear Regression: 0.9988\n",
      "**Final Tuned R¬≤ Score of Linear Regression: 0.9988**\n",
      "Final Mean Squared Error: 24465.6911\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"üöÄ STARTING REGRESSION MODEL PIPELINE üöÄ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 6a. Split data for Regression using TimeSeriesSplit ---\n",
    "# For regression on time series, it's better not to shuffle the data.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train_reg, X_test_reg = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_reg, y_test_reg = y_reg.iloc[train_index], y_reg.iloc[test_index]\n",
    "\n",
    "# --- 6b. Find Best Baseline Regression Model ---\n",
    "print(\"\\n--- Step 6a: Evaluating Regression Models with Normal Parameters ---\")\n",
    "reg_models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42)\n",
    "}\n",
    "baseline_reg_scores = {}\n",
    "for name, model in reg_models.items():\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), ('regressor', model)])\n",
    "    pipeline.fit(X_train_reg, y_train_reg)\n",
    "    y_pred_reg = pipeline.predict(X_test_reg)\n",
    "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "    baseline_reg_scores[name] = r2\n",
    "    print(f\"  R¬≤ Score of {name}: {r2:.4f}\")\n",
    "\n",
    "best_reg_model_name = max(baseline_reg_scores, key=baseline_reg_scores.get)\n",
    "print(f\"\\nüèÜ Best Baseline Regressor: '{best_reg_model_name}'\")\n",
    "\n",
    "# --- 6c. Hyperparameter Tune the Best Regression Model ---\n",
    "print(f\"\\n--- Step 6b: Performing Hyperparameter Tuning for {best_reg_model_name} ---\")\n",
    "reg_param_grids = {\n",
    "    \"Linear Regression\": {},\n",
    "    \"Random Forest\": {'regressor__n_estimators': [100, 200], 'regressor__max_depth': [10, 20]},\n",
    "    \"XGBoost\": {'regressor__n_estimators': [100, 200], 'regressor__learning_rate': [0.05, 0.1], 'regressor__max_depth': [3, 5]}\n",
    "}\n",
    "chosen_reg_model = reg_models[best_reg_model_name]\n",
    "chosen_reg_grid = reg_param_grids[best_reg_model_name]\n",
    "final_reg_pipeline = Pipeline([('scaler', StandardScaler()), ('regressor', chosen_reg_model)])\n",
    "grid_search_reg = GridSearchCV(final_reg_pipeline, chosen_reg_grid, cv=tscv, n_jobs=-1, scoring='r2', verbose=1)\n",
    "grid_search_reg.fit(X, y_reg) # Use full dataset for CV tuning in regression\n",
    "\n",
    "# --- 6d. Final Evaluation of Tuned Regression Model ---\n",
    "print(\"\\n--- Step 6c: Final Evaluation of Tuned Regression Model ---\")\n",
    "final_reg_model = grid_search_reg.best_estimator_\n",
    "y_pred_reg_final = final_reg_model.predict(X_test_reg)\n",
    "final_r2 = r2_score(y_test_reg, y_pred_reg_final)\n",
    "final_mse = mean_squared_error(y_test_reg, y_pred_reg_final)\n",
    "print(f\"Initial R¬≤ Score of {best_reg_model_name}: {baseline_reg_scores[best_reg_model_name]:.4f}\")\n",
    "print(f\"**Final Tuned R¬≤ Score of {best_reg_model_name}: {final_r2:.4f}**\")\n",
    "print(f\"Final Mean Squared Error: {final_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f86b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_historical_data():\n",
    "    \"\"\"\n",
    "    Takes a random sample from the unseen test set, makes a prediction,\n",
    "    and displays the results. This demonstrates the model's functionality\n",
    "    without needing live data or user input.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"ü§ñ PREDICTING ON A SAMPLE FROM THE HISTORICAL TEST SET ü§ñ\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    try:\n",
    "        # --- 4a. Select a Random Unseen Data Point ---\n",
    "        # We take one random row from our test set (X_test).\n",
    "        # The model has not been trained on this data.\n",
    "        sample_features = X_test.sample(1, random_state=42)\n",
    "        \n",
    "        print(\"Using the following random data point from the test set:\")\n",
    "        # .T transposes the data for better readability\n",
    "        print(sample_features.T)\n",
    "\n",
    "        # --- 4b. Make Predictions ---\n",
    "        # Scale the features using the scaler we trained in Part 1\n",
    "        sample_features_scaled = scaler.transform(sample_features)\n",
    "\n",
    "        # Classification Prediction using the final tuned model from Part 3\n",
    "        cls_pred_raw = final_cls_model.predict(sample_features_scaled)\n",
    "        cls_proba = final_cls_model.predict_proba(sample_features_scaled)\n",
    "        cls_pred = \"UP üîº\" if cls_pred_raw[0] == 1 else \"DOWN üîΩ\"\n",
    "        confidence = cls_proba[0][cls_pred_raw[0]]\n",
    "\n",
    "        # Regression Prediction (using the unscaled features for tree-based models)\n",
    "        # using the final tuned model from Part 2\n",
    "        reg_pred = final_reg_model.predict(sample_features)\n",
    "\n",
    "        # --- 4c. Display Results ---\n",
    "        print(\"\\n--- PREDICTION RESULTS ---\")\n",
    "        print(f\"üß≠ Predicted Next Hour's Movement: {cls_pred} (Confidence: {confidence:.2%})\")\n",
    "        print(f\"üí≤ Predicted Next Hour's Close Price: ${reg_pred[0]:,.2f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during the prediction process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b8206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "ü§ñ PREDICTING ON A SAMPLE FROM THE HISTORICAL TEST SET ü§ñ\n",
      "==================================================\n",
      "\n",
      "An error occurred during the prediction process: name 'X_test' is not defined\n"
     ]
    }
   ],
   "source": [
    "predict_on_historical_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde00557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
